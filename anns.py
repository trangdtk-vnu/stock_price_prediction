# -*- coding: utf-8 -*-
"""anns.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bK23QThSaqSJtoWL_wiEGyfb6bGdVYzc
"""

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
import torch
import torch.nn as nn
import torch.optim as optim
from copy import deepcopy
import random
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt

"""# custom_loss function"""

class EnhancedSignAgreementLoss(nn.Module):
    def __init__(self, loss_penalty):
        super(EnhancedSignAgreementLoss, self).__init__()
        self.loss_penalty = loss_penalty

    def forward(self, y_true, y_pred):
        # Check if signs of y_true and y_pred are the same (including zero)
        same_sign = torch.eq(torch.sign(y_true), torch.sign(y_pred))

        # Calculate the residual (difference between y_true and y_pred)
        residual = y_true - y_pred

        # Compute the loss based on the condition
        loss = torch.where(same_sign,
                           torch.square(residual),
                           torch.square(residual) + self.loss_penalty)

        # Return the mean loss
        return torch.mean(loss)

"""# Simple RNNs"""

# Define RNNs
class SimpleRNN(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(SimpleRNN, self).__init__()
        # Initialize the RNN layer
        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)
        # Initialize the fully connected layer
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        # x shape: (batch, seq_length, features)
        out, _ = self.rnn(x)  # Outputs from all timesteps
        # Use the output from the last timestep
        out = self.fc(out[:, -1, :])  # Last timestep output to fully connected layer
        return out

def rnns(model, train_loader, val_loader, epochs, optimizer, loss_function):
    best_val_loss = float('inf')
    best_model = None

    for epoch in range(epochs):
        model.train()
        for inputs, labels in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = loss_function(outputs, labels)
            loss.backward()
            optimizer.step()

        # Validation phase
        model.eval()
        total_val_loss = 0
        with torch.no_grad():
            for inputs, labels in val_loader:
                outputs = model(inputs)
                val_loss = loss_function(outputs, labels).item()
                total_val_loss += val_loss

        avg_val_loss = total_val_loss / len(val_loader)
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            best_model = model.state_dict()  # Update the best model

        if epoch % 10 == 0:
            print(f'Epoch {epoch+1}, Train Loss: {loss.item()}, Val Loss: {avg_val_loss}')

    # Save the best model state to a file
    torch.save(best_model, 'path_to_saved_model.pt')
    return best_val_loss

import matplotlib.pyplot as plt
import torch

def plots_rnns(model, data_loader):
    model.eval()
    sign_predictions = []
    sign_actuals = []
    with torch.no_grad():
        for X_batch, y_batch in data_loader:
            outputs = model(X_batch)
            # Convert outputs and labels to signs (-1, 0, 1)
            sign_pred = torch.sign(outputs).numpy()  # Convert tensor to numpy after taking sign
            sign_act = torch.sign(y_batch).numpy()   # Convert tensor to numpy after taking sign

            sign_predictions.extend(sign_pred.flatten())  # Flatten in case outputs are multidimensional
            sign_actuals.extend(sign_act.flatten())       # Flatten in case outputs are multidimensional

    # Plotting the signs
    plt.figure(figsize=(10, 5))
    plt.plot(sign_actuals, label='Actual Signs')
    plt.plot(sign_predictions, label='Predicted Signs', alpha=0.7)
    plt.title('Comparison of Actual and Predicted Signs')
    plt.xlabel('Samples')
    plt.ylabel('Sign of Target Value')
    plt.legend()
    plt.show()

def create_sequences_rnns(X, y, time_steps):
    Xs, ys = [], []
    for i in range(len(X) - time_steps):
        Xs.append(X[i:(i + time_steps)])
        ys.append(y[i + time_steps])
    return np.array(Xs), np.array(ys)

def random_search_hyperparameters_rnns(train_loader, val_loader, num_trials, epochs, input_size, output_size):
    best_loss = float('inf')
    best_settings = {}

    for _ in range(num_trials):
        # Randomly choosing hyperparameters
        learning_rate = 10**np.random.uniform(-4, -1)
        loss_penalty = np.random.uniform(1.0, 5.0)
        hidden_size = int(np.random.choice([20, 40, 80]))
        num_layers = int(np.random.choice([1, 2, 3]))

        # Model and optimizer setup
        model = SimpleRNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, output_size=output_size)
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        loss_function = EnhancedSignAgreementLoss(loss_penalty=loss_penalty)

        # Train and validate the model
        val_loss = rnns(model, train_loader, val_loader, epochs, optimizer, loss_function)

        # Update best loss
        if val_loss < best_loss:
            best_loss = val_loss
            best_settings = {
                'learning_rate': learning_rate,
                'loss_penalty': loss_penalty,
                'hidden_size': hidden_size,
                'num_layers': num_layers
            }

    return best_settings

"""# LSTM"""

class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(LSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        # Initialize hidden and cell states
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)

        # Forward propagate LSTM
        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)

        # Decode the hidden state of the last time step
        out = self.fc(out[:, -1, :])
        return out

def lstm(model, train_loader, val_loader, epochs, optimizer, loss_function):
    """
    Trains an LSTM model and evaluates it on a validation set.

    Args:
        model (torch.nn.Module): The LSTM model to train.
        train_loader (DataLoader): DataLoader for the training set.
        val_loader (DataLoader): DataLoader for the validation set.
        epochs (int): Number of epochs to train.
        optimizer (torch.optim.Optimizer): Optimizer to use for training.
        loss_function (callable): Loss function to use for training.

    Returns:
        tuple: Tuple containing the state_dict of the best model (by validation loss)
               and the best validation loss observed.
    """
    best_val_loss = float('inf')
    best_model = None

    for epoch in range(epochs):
        model.train()
        total_train_loss = 0
        for inputs, labels in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = loss_function(outputs, labels)
            loss.backward()
            optimizer.step()
            total_train_loss += loss.item()

        avg_train_loss = total_train_loss / len(train_loader)

        model.eval()
        total_val_loss = 0
        with torch.no_grad():
            for inputs, labels in val_loader:
                outputs = model(inputs)
                val_loss = loss_function(outputs, labels).item()
                total_val_loss += val_loss

        avg_val_loss = total_val_loss / len(val_loader)
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            best_model = model.state_dict()  # Update the best model to the current state

        if epoch % 10 == 0:
            print(f'Epoch {epoch+1}, Train Loss: {avg_train_loss}, Val Loss: {avg_val_loss}')

    # Communicate final best validation loss and return the best model
    print(f'Final Best Validation Loss: {best_val_loss}')
    return best_model, best_val_loss

import matplotlib.pyplot as plt
import torch

def plots_lstm(model, data_loader):
    model.eval()
    sign_predictions = []
    sign_actuals = []
    with torch.no_grad():
        for inputs, labels in data_loader:
            outputs = model(inputs)
            # Get the signs of the outputs and actual labels
            sign_pred = torch.sign(outputs).numpy()  # Convert tensor to numpy after taking sign
            sign_act = torch.sign(labels).numpy()    # Convert tensor to numpy after taking sign

            sign_predictions.extend(sign_pred.flatten())  # Flatten in case outputs are multidimensional
            sign_actuals.extend(sign_act.flatten())       # Flatten in case labels are multidimensional

    # Plotting the signs
    plt.figure(figsize=(10, 5))
    plt.plot(sign_actuals, label='Actual Signs', linewidth=2)
    plt.plot(sign_predictions, label='Predicted Signs', alpha=0.7, linestyle='--')
    plt.title('Comparison of Actual and Predicted Signs')
    plt.xlabel('Sample Index')
    plt.ylabel('Sign of Value')
    plt.legend()
    plt.show()

def random_search_hyperparameters_lstm(num_trials, epochs, train_loader, val_loader, input_size, output_size):
    best_loss = float('inf')
    best_settings = {}

    for _ in range(num_trials):
        # Randomly choosing hyperparameters
        learning_rate = 10**np.random.uniform(-4, -1)
        loss_penalty = np.random.uniform(1.0, 5.0)
        hidden_size = int(np.random.choice([20, 40, 80]))
        num_layers = int(np.random.choice([1, 2, 3]))

        # Model and optimizer setup
        model = LSTMModel(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, output_size=output_size)
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        loss_function = EnhancedSignAgreementLoss(loss_penalty=loss_penalty)

        # Train and validate the model
        _, val_loss = lstm(model, train_loader, val_loader, epochs, optimizer, loss_function)

        # Update best loss
        if val_loss < best_loss:
            best_loss = val_loss
            best_settings = {
                'learning_rate': learning_rate,
                'loss_penalty': loss_penalty,
                'hidden_size': hidden_size,
                'num_layers': num_layers
            }

    return best_settings

"""# ANFIS"""

class EVOLUTIONARY_ANFIS:
    def __init__(self, functions, generations, offsprings, mutationRate, learningRate, chance, ruleComb, loss_penalty):
        self.functions = functions
        self.generations = generations
        self.offsprings = offsprings
        self.mutationRate = mutationRate
        self.learningRate = learningRate
        self.chance = chance  # 50 percent chance of changing std.
        self.ruleComb = ruleComb
        self._noParam = 2
        self.loss_function = EnhancedSignAgreementLoss(loss_penalty)

    def gaussian(self,x, mu, sig):
        return np.exp((-np.power(x - mu, 2.) / (2 * np.power(sig, 2.))))

    def initialize(self,X):
        functions = self.functions
        noParam = self._noParam
        ruleComb = self.ruleComb
        inputs = np.zeros((X.shape[1],X.shape[0],functions))
        Ant = np.zeros((noParam,X.shape[1],X.shape[0],functions))
        L1 = np.zeros((X.shape[1],X.shape[0],functions))
        if ruleComb == "simple":
            L2 = np.zeros((X.shape[0],functions))
        elif ruleComb == "complete":
            rules = X.shape[1]**functions
            L2 = np.zeros((X.shape[0],rules))
        return inputs, Ant, L1, L2

    def mutation(self,arr):
        mutationRate = self.mutationRate
        learningRate = self.learningRate
        chance = self.chance
        temp = np.asarray(arr)   # Cast to numpy array
        mean = temp[0]
        meanShape = mean.shape
        std = temp[1]
        stdShape = std.shape
        mean = mean.flatten()    # Flatten to 1D
        std = std.flatten()    # Flatten to 1D
        num = int(mutationRate*mean.size) # number of elements to get
        if random.uniform(0,1)>chance:
            inds = np.random.choice(mean.size, size=num)   # Get random indices
            mean[inds] -= np.random.uniform(0,1,size=num)*learningRate        # Fill with something
            mean = mean.reshape(meanShape)                     # Restore original shape
            std = std.reshape(stdShape)
        else:
            inds = np.random.choice(std.size, size=num)   # Get random indices
            std[inds] -= np.random.uniform(0,1,size=num)*learningRate        # Fill with something
            std = std.reshape(stdShape)                     # Restore original shape
            std = np.where(std==0, 0.0001, std) #standard deviation cannot be zero
            #temp = np.where(temp<=0, 0.0001, temp)
            #temp = np.where(temp>=1, 0.9999, temp)

            mean = mean.reshape(meanShape)
        temp[0] = mean
        temp[1] = std
        return temp

    def init_population(self,X):
        noParam = self._noParam
        functions = self.functions
        offsprings = self.offsprings
        bestParam = np.random.rand(noParam,X.shape[1],functions)
        parentParam = deepcopy(bestParam)
        popParam = []
        for i in range(offsprings):
            popParam.append(self.mutation(parentParam))
        return popParam

    def init_model(self,model=LinearRegression()):
        models = []
        for i in range(self.functions):
                models.append(model)
        return models

    def forwardPass(self,param,X,inputs,Ant,L1,L2,functions):
        noParam = self._noParam

        for i in range(X.shape[1]):   #input variables
            inputs[i] = np.repeat(X[:,i].reshape(-1,1),functions,axis=1)

        for ii in range(noParam):   #Anticedent parameters
            for i in range(X.shape[1]):
                Ant[ii] = np.repeat(param[ii][i,:].reshape(1,-1),X.shape[0],axis=0)

        for i in range(X.shape[1]):  #Membership values using Gaussian membership function
            L1[i,:,:] = self.gaussian(x=inputs[i],mu=Ant[0][i],sig=Ant[1][i])

        for j in range(functions):      #rule
            for i in range(1,X.shape[1]):
                L2[:,j] = (L1[i-1,:,j]*L1[i,:,j])#+(L1[i-1,:,j]+L1[i,:,j])

        summ = np.sum(L2,axis=1).reshape(-1,1) #Weights normalization
        summation = np.repeat(summ,functions,axis=1)
        np.clip(summation, a_min=1e-6, a_max=None)
        L3 = L2/summation
        L3 = np.round(L3,5)
        #Errorcheck = np.sum(L3,axis=1)

        consequent = X
        L4 = np.zeros((functions,X.shape[0],X.shape[1]))
        for i in range (functions):
            L4[i] = consequent
            L4[i] = L4[i]*L3[:,i].reshape(-1,1)
        return L1,L2,L3,L4

    def linear_fit(self, L3, L4, X, y, functions, models):
        pred_train = np.zeros((X.shape[0], functions))
        for i in range(functions):
            models[i].fit(L4[i], y)
            predTemp = models[i].predict(L4[i])
            # Ensure predTemp is two-dimensional
            if predTemp.ndim == 1:
                predTemp = predTemp.reshape(-1, 1)
            pred_train[:, i] = predTemp[:, 0]
        pred_train = pred_train * L3  # Consequent function output * normalized weights
        pred_train = np.sum(pred_train, axis=1)
        return pred_train, models

    def linear_predict(self,L3,L4,X,functions,Trained_models):
        pred_test = np.zeros((X.shape[0],functions))
        for i in range(functions):
            predTemp = Trained_models[i].predict(L4[i]).reshape(-1,1)
            pred_test[:,i] = predTemp[:,0]
        pred_test = pred_test*L3 #consequent function output * normalized weights
        pred_test = np.sum(pred_test,axis=1)
        return pred_test

    def fit(self, X_train, y_train, X_test=None, y_test=None, optimize_test_data=False):
        popParam = self.init_population(X_train)
        inputsTrain, AntTrain, L1Train, L2Train = self.initialize(X_train)
        if optimize_test_data:
            inputsTest, AntTest, L1Test, L2Test = self.initialize(X_test)
        models = self.init_model()
        bestParam = popParam[0]

        for gen in range(self.generations):
            parentParam = deepcopy(bestParam)
            popParam[0] = deepcopy(bestParam)
            for ii in range(1, self.offsprings):
                mut = self.mutation(parentParam)
                popParam[ii] = deepcopy(mut)

            PopulationError = []
            bestModelLst = []
            for i in range(len(popParam)):
                L1, L2, L3, L4 = self.forwardPass(popParam[i], X_train, inputsTrain, AntTrain, L1Train, L2Train, self.functions)
                pred_train, Trained_models = self.linear_fit(L3, L4, X_train, y_train, self.functions, models)

                # Here we use the custom loss function instead of RMSE
                train_loss = self.loss_function(torch.tensor(pred_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))

                if optimize_test_data:
                    L1, L2, L3, L4 = self.forwardPass(popParam[i], X_test, inputsTest, AntTest, L1Test, L2Test, self.functions)
                    pred_test = self.linear_predict(L3, L4, X_test, self.functions, Trained_models)
                    test_loss = self.loss_function(torch.tensor(pred_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32))

                    PopulationError.append((train_loss + test_loss) / 2)
                    bestModelLst.append(Trained_models)
                else:
                    PopulationError.append(train_loss)
                    bestModelLst.append(Trained_models)

            bestParamIndex = np.argmin(PopulationError)
            bestParam = deepcopy(popParam[bestParamIndex])
            bestModel = bestModelLst[bestParamIndex]
            print(f"Generation {gen}, Loss: {PopulationError[bestParamIndex]}")

        return bestParam, bestModel

    def predict(self,X,bestParam,bestModel):
        functions = self.functions
        inputs,Ant,L1,L2 = self.initialize(X)
        L1,L2,L3,L4 = self.forwardPass(bestParam,X,inputs,Ant,L1,L2,functions)
        pred = self.linear_predict(L3,L4,X,functions,bestModel)
        return pred

def grid_search_hyperparameters_anfis(X_train, y_train, X_val, y_val, param_grid):
    best_loss = float('inf')
    best_hyperparameters = {}

    for functions in param_grid['functions']:
        for generations in param_grid['generations']:
            for offsprings in param_grid['offsprings']:
                for mutationRate in param_grid['mutationRate']:
                    for learningRate in param_grid['learningRate']:
                        for chance in param_grid['chance']:
                            for ruleComb in param_grid['ruleComb']:
                                for loss_penalty in param_grid['loss_penalty']:
                                    anfis_model = EVOLUTIONARY_ANFIS(
                                        functions=functions,
                                        generations=generations,
                                        offsprings=offsprings,
                                        mutationRate=mutationRate,
                                        learningRate=learningRate,
                                        chance=chance,
                                        ruleComb=ruleComb,
                                        loss_penalty=loss_penalty
                                    )
                                    anfis_model.fit(X_train, y_train)
                                    current_loss = anfis_model.evaluate(X_val, y_val)

                                    if current_loss < best_loss:
                                        best_loss = current_loss
                                        best_hyperparameters = {
                                            'functions': functions,
                                            'generations': generations,
                                            'offsprings': offsprings,
                                            'mutationRate': mutationRate,
                                            'learningRate': learningRate,
                                            'chance': chance,
                                            'ruleComb': ruleComb,
                                            'loss_penalty': loss_penalty
                                        }

    return best_hyperparameters

import matplotlib.pyplot as plt
import numpy as np  # Make sure to import numpy if not already done

def plots_anfis(actuals, predictions, title='Predictions vs Actual Signs'):
    # Convert actual and predicted values to their signs
    sign_actuals = np.sign(actuals)
    sign_predictions = np.sign(predictions)

    # Plotting the signs
    plt.figure(figsize=(10, 5))
    plt.plot(sign_actuals, label='Actual Signs', linestyle='-', marker='o', markersize=4)
    plt.plot(sign_predictions, label='Predicted Signs', linestyle='--', alpha=0.7)
    plt.title(title)
    plt.xlabel('Sample Index')
    plt.ylabel('Sign of Target Value')
    plt.legend()
    plt.show()