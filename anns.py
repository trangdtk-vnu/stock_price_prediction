# -*- coding: utf-8 -*-
"""anns.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bK23QThSaqSJtoWL_wiEGyfb6bGdVYzc
"""

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
import torch
import torch.nn as nn
import torch.optim as optim
from copy import deepcopy
import random
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import copy
from scipy.stats import truncnorm
from torch.optim import Adam
from torch.nn import MSELoss
from torch.utils.data import TensorDataset
from torch.nn.functional import mse_loss as MSELoss

"""# custom_loss function"""

class EnhancedSignAgreementLoss(nn.Module):
    def __init__(self, loss_penalty):
        super(EnhancedSignAgreementLoss, self).__init__()
        self.loss_penalty = loss_penalty

    def forward(self, y_true, y_pred):
        # Check if signs of y_true and y_pred are the same (including zero)
        same_sign = torch.eq(torch.sign(y_true), torch.sign(y_pred))

        # Calculate the residual (difference between y_true and y_pred)
        residual = y_true - y_pred

        # Compute the loss based on the condition
        loss = torch.where(same_sign,
                           torch.square(residual),
                           torch.square(residual) + self.loss_penalty)

        # Return the mean loss
        return torch.mean(loss)

"""# RMSE"""

# Define RMSE loss function
class RMSELoss(nn.Module):
    def __init__(self):
        super().__init__()
        self.mse = nn.MSELoss()

    def forward(self, pred, actual):
        return torch.sqrt(self.mse(pred, actual))

"""# Simple RNNs

## Create sequence
"""

# Create sequences for RNNs
def create_sequences_rnns(X, y, time_steps):
    Xs, ys = [], []
    for i in range(len(X) - time_steps):
        Xs.append(X[i:(i + time_steps)])
        ys.append(y[i + time_steps])
    return np.array(Xs), np.array(ys)

"""## RNNs architecture"""

class SimpleRNN(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(SimpleRNN, self).__init__()
        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        out, _ = self.rnn(x)  # Outputs from all timesteps
        out = self.fc(out[:, -1, :])  # Last timestep output to fully connected layer
        return out.squeeze()  # Remove extra dimension to match target shape

def rnns(model, train_loader, val_loader, epochs, optimizer, loss_function):
    best_val_loss = float('inf')
    best_model_state = None

    for epoch in range(epochs):
        model.train()
        for inputs, labels in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            if outputs.dim() > 1 and outputs.shape[1] == 1:
                outputs = outputs.squeeze(1)
            loss = loss_function(outputs, labels)
            loss.backward()
            optimizer.step()

        model.eval()
        total_val_loss = 0
        with torch.no_grad():
            for inputs, labels in val_loader:
                outputs = model(inputs)
                if outputs.dim() > 1 and outputs.shape[1] == 1:
                    outputs = outputs.squeeze(1)
                val_loss = loss_function(outputs, labels)
                total_val_loss += val_loss

        avg_val_loss = total_val_loss / len(val_loader)
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            best_model_state = model.state_dict()  # Save the best model state

        if epoch % 10 == 0:
            print(f'Epoch {epoch+1}, Train Loss: {loss.item()}, Val Loss: {avg_val_loss}')

    return best_model_state, best_val_loss

"""## GA optimiser RMSE"""

class GAoptimizer_rnns_RMSE:
    def __init__(self, input_size, output_size, train_loader, val_loader, epochs, population_size, max_generations):
        self.input_size = input_size
        self.output_size = output_size
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.epochs = epochs
        self.population_size = population_size
        self.max_generations = max_generations
        self.population = self.initialize_population()
        self.best_model_state = None  # Stores the state dict of the best model
        self.best_validation_loss = float('inf')  # Initialize with a very high value

    def initialize_population(self):
        population = []
        for _ in range(self.population_size):
            individual = {
                'hidden_size': int(np.random.choice([50, 100, 150])),
                'num_layers': int(np.random.choice([1, 2, 3])),
                'lr': np.random.uniform(0.001, 0.01)
            }
            population.append(individual)
        return population

    def select_parents(self):
        selected = []
        for _ in range(int(self.population_size / 2)):
            competitors = np.random.choice(self.population, 2)
            winner = min(competitors, key=lambda ind: ind['fitness'])
            selected.append(winner)
        return selected

    def crossover(self, parent1, parent2):
        child1, child2 = copy.deepcopy(parent1), copy.deepcopy(parent2)
        crossover_point = np.random.choice(list(child1.keys()))
        child1[crossover_point], child2[crossover_point] = child2[crossover_point], child1[crossover_point]
        return child1, child2

    def mutate(self, individual):
        mutation_property = np.random.choice(list(individual.keys()))
        if mutation_property == 'hidden_size':
            individual[mutation_property] = int(np.random.choice([50, 100, 150]))
        elif mutation_property == 'num_layers':
            individual[mutation_property] = int(np.random.choice([1, 2, 3]))
        elif mutation_property == 'lr':
            individual[mutation_property] = np.random.uniform(0.0001, 0.01)
        return individual

    def evaluate_individual(self, individual):
        model = SimpleRNN(input_size=self.input_size, hidden_size=individual['hidden_size'], num_layers=individual['num_layers'], output_size=self.output_size)
        optimizer = optim.Adam(model.parameters(), lr=individual['lr'])
        loss_function = RMSELoss()

        # The rnns function returns a tuple (best_model_state, best_val_loss)
        best_model_state, validation_loss = rnns(model, self.train_loader, self.val_loader, self.epochs, optimizer, loss_function)

        # Check if the current validation loss is better than the best recorded validation loss
        if validation_loss < self.best_validation_loss:
            self.best_validation_loss = validation_loss
            self.best_model_state = best_model_state  # Save the best model state

        return validation_loss

    def evolve(self):
        for generation in range(self.max_generations):
            print(f'Generation: {generation + 1}')
            for individual in self.population:
                individual['fitness'] = self.evaluate_individual(individual)
                print(f'Individual fitness: {individual["fitness"]}')

            new_population = []
            parents = self.select_parents()
            for parent1, parent2 in zip(parents[::2], parents[1::2]):
                child1, child2 = self.crossover(parent1, parent2)
                new_population.extend([self.mutate(child1), self.mutate(child2)])

            self.population = new_population
            print(f'End of Generation {generation + 1}')

"""## GA optimiser custom loss function"""

class GAoptimizer_rnns_custom:
    def __init__(self, input_size, output_size, train_loader, val_loader, epochs, population_size, max_generations, loss_penalty):
        self.input_size = input_size
        self.output_size = output_size
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.epochs = epochs
        self.population_size = population_size
        self.max_generations = max_generations
        self.loss_penalty = loss_penalty
        self.population = self.initialize_population()
        self.best_model_state = None  # To save the state of the best model
        self.best_validation_loss = float('inf')  # Initialize with a high value

    def initialize_population(self):
        # Initializes a population with random hyperparameters
        return [{
            'hidden_size': int(np.random.choice([50, 100, 150])),
            'num_layers': int(np.random.choice([1, 2, 3])),
            'lr': np.random.uniform(0.0001, 0.01)
        } for _ in range(self.population_size)]

    def select_parents(self):
        # Selects parents using tournament selection
        return [min(np.random.choice(self.population, 2), key=lambda ind: ind['fitness']) for _ in range(int(self.population_size / 2))]

    def crossover(self, parent1, parent2):
        # Performs a crossover between two parents
        child1, child2 = deepcopy(parent1), deepcopy(parent2)
        crossover_point = np.random.choice(list(child1.keys()))
        for key in list(child1.keys())[list(child1.keys()).index(crossover_point):]:
            child1[key], child2[key] = child2[key], child1[key]
        return child1, child2

    def mutate(self, individual):
        # Mutates an individual's hyperparameters
        mutation_property = np.random.choice(list(individual.keys()))
        if mutation_property == 'hidden_size':
            individual[mutation_property] = int(np.random.choice([50, 100, 150]))
        elif mutation_property == 'num_layers':
            individual[mutation_property] = int(np.random.choice([1, 2, 3]))
        elif mutation_property == 'lr':
            individual[mutation_property] = np.random.uniform(0.0001, 0.01)
        return individual

    def evaluate_individual(self, individual):
        # Evaluates an individual by training an RNN model
        model = SimpleRNN(input_size=self.input_size, hidden_size=individual['hidden_size'], num_layers=individual['num_layers'], output_size=self.output_size)
        optimizer = optim.Adam(model.parameters(), lr=individual['lr'])
        loss_function = EnhancedSignAgreementLoss(loss_penalty=self.loss_penalty)

        validation_loss = rnns(model, self.train_loader, self.val_loader, self.epochs, optimizer, loss_function)
        if validation_loss < self.best_validation_loss:
            self.best_validation_loss = validation_loss
            self.best_model_state = model.state_dict()
        return validation_loss

    def evolve(self):
        # Evolves the population using GA
        for generation in range(self.max_generations):
            print(f'Generation: {generation + 1}')
            for individual in self.population:
                individual['fitness'] = self.evaluate_individual(individual)

            new_population = self.select_parents()
            while len(new_population) < self.population_size:
                parent1, parent2 = np.random.choice(new_population, 2, replace=False)
                child1, child2 = self.crossover(parent1, parent2)
                new_population.extend([self.mutate(child1), self.mutate(child2)])

            self.population = new_population

    def get_best_model(self):
        # Retrieves the best model's state for later use
        model = SimpleRNN(input_size=self.input_size, hidden_size=self.best_model_state['hidden_size'], num_layers=self.best_model_state['num_layers'], output_size=self.output_size)
        model.load_state_dict(self.best_model_state)
        return model

"""# ANFIS

## ANFIS RMSE
"""

class ANFIS_RMSE(nn.Module):
    def __init__(self, functions, input_dim):
        super(ANFIS_RMSE, self).__init__()
        self.functions = functions
        self.input_dim = input_dim
        self.mu = nn.Parameter(torch.randn(input_dim, functions))
        self.sigma = nn.Parameter(torch.randn(input_dim, functions))
        self.linear = nn.Linear(functions, 1)
        self.best_model_state = None  # To save the best model state

    def forward(self, x):
        batch_size = x.size(0)
        mu = self.mu.expand(batch_size, -1, -1)
        sigma = torch.exp(self.sigma).expand(batch_size, -1, -1)
        x = x.unsqueeze(2).expand(-1, -1, self.functions)
        gaussians = torch.exp(-torch.pow(x - mu, 2) / (2 * sigma.pow(2)))
        t_norm = torch.prod(gaussians, dim=1)
        y = self.linear(t_norm)
        return y.squeeze(1)

    def fit(self, X_train, y_train, X_val, y_val, epochs, lr):
        train_dataset = TensorDataset(X_train, y_train)
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        val_dataset = TensorDataset(X_val, y_val)
        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
        optimizer = optim.Adam(self.parameters(), lr=lr)
        loss_function = nn.MSELoss()
        best_val_loss = float('inf')

        for epoch in range(epochs):
            self.train()
            total_train_loss = 0.0
            for x_batch, y_batch in train_loader:
                optimizer.zero_grad()
                outputs = self.forward(x_batch)
                loss = loss_function(outputs, y_batch)
                loss.backward()
                optimizer.step()
                total_train_loss += loss.item() * x_batch.size(0)

            avg_train_loss = total_train_loss / len(train_loader.dataset)

            # Validation phase
            self.eval()
            total_val_loss = 0.0
            with torch.no_grad():
                for x_batch, y_batch in val_loader:
                    outputs = self.forward(x_batch)
                    val_loss = loss_function(outputs, y_batch)
                    total_val_loss += val_loss.item() * x_batch.size(0)

            avg_val_loss = total_val_loss / len(val_loader.dataset)

            # Update best model if validation loss improved
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                self.best_model_state = self.state_dict()

            # Print losses
            if epoch % 10 == 0:
                print(f"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}")

        print(f"Best Validation Loss: {best_val_loss:.4f}")

    def predict(self, X):
        self.load_state_dict(self.best_model_state)
        self.eval()
        with torch.no_grad():
            return self.forward(X)

"""## GA ANFIS RMSE"""

class GAoptimizer_ANFIS_RMSE:
    def __init__(self, model_class, input_dim, X_train, y_train, X_val, y_val, epochs, population_size, max_generations):
        self.model_class = model_class
        self.input_dim = input_dim
        self.X_train = X_train
        self.y_train = y_train
        self.X_val = X_val
        self.y_val = y_val
        self.epochs = epochs
        self.population_size = population_size
        self.max_generations = max_generations
        self.population = self.initialize_population()
        self.best_validation_loss = float('inf')  # Initialize with a very high value
        self.best_model_state = None  # Placeholder for the best model's state dictionary

    def initialize_population(self):
        population = []
        for _ in range(self.population_size):
            individual = {
                'functions': int(np.random.choice([5, 10, 15])),  # Ranges for number of functions
                'lr': np.random.uniform(0.001, 0.01)  # Learning rate
            }
            population.append(individual)
        return population

    def evaluate_individual(self, individual):
        model = self.model_class(functions=individual['functions'], input_dim=self.input_dim)
        optimizer = optim.Adam(model.parameters(), lr=individual['lr'])
        loss_function = MSELoss()

        dataset = TensorDataset(self.X_train, self.y_train)
        train_loader = DataLoader(dataset, batch_size=32, shuffle=True)

        model.train()
        for epoch in range(self.epochs):
            for x_batch, y_batch in train_loader:
                optimizer.zero_grad()
                outputs = model(x_batch)
                loss = loss_function(outputs, y_batch)
                loss.backward()
                optimizer.step()

        model.eval()
        with torch.no_grad():
            outputs = model(self.X_val)
            val_loss = loss_function(outputs, self.y_val).item()

        if val_loss < self.best_validation_loss:
            self.best_validation_loss = val_loss
            self.best_model_state = model.state_dict()

        return val_loss

    def crossover(self, parent1, parent2):
        child1, child2 = deepcopy(parent1), deepcopy(parent2)
        if np.random.rand() > 0.5:
            child1['functions'], child2['functions'] = child2['functions'], child1['functions']
        return child1, child2

    def mutate(self, individual):
        if np.random.rand() < 0.1:
            individual['functions'] = int(np.random.choice([5, 10, 15]))
        if np.random.rand() < 0.1:
            individual['lr'] = np.random.uniform(0.001, 0.01)
        return individual

    def evolve(self):
        for generation in range(self.max_generations):
            print(f'Generation {generation+1}')
            fitnesses = [self.evaluate_individual(ind) for ind in self.population]
            ranked_population = sorted(zip(self.population, fitnesses), key=lambda x: x[1])
            self.population = [ind for ind, fit in ranked_population[:int(self.population_size / 2)]]

            while len(self.population) < self.population_size:
                parent1, parent2 = np.random.choice([ind for ind, _ in ranked_population[:int(self.population_size / 2)]], 2, replace=False)
                child1, child2 = self.crossover(parent1, parent2)
                self.population.extend([self.mutate(child1), self.mutate(child2)])

            print(f'Best Validation Loss this Generation: {ranked_population[0][1]}')

        best_parameters = ranked_population[0][0]
        self.load_best_model()
        print(f"Best parameters found: {best_parameters}")
        return best_parameters

    def load_best_model(self):
        self.model_class().load_state_dict(self.best_model_state)
        print("Loaded best model.")

"""## ANFIS custom loss function"""

class ANFIS_CustomLoss(nn.Module):
    def __init__(self, functions, input_dim, loss_penalty):
        super(ANFIS_CustomLoss, self).__init__()
        self.functions = functions
        self.input_dim = input_dim
        self.loss_penalty = loss_penalty
        self.mu = nn.Parameter(torch.randn(input_dim, functions))
        self.sigma = nn.Parameter(torch.randn(input_dim, functions))
        self.linear = nn.Linear(functions, 1)

    def forward(self, x):
        batch_size = x.size(0)
        mu = self.mu.expand(batch_size, -1, -1)
        sigma = torch.exp(self.sigma).expand(batch_size, -1, -1)
        x = x.unsqueeze(2).expand(-1, -1, self.functions)
        gaussians = torch.exp(-torch.pow(x - mu, 2) / (2 * sigma.pow(2)))
        t_norm = torch.prod(gaussians, dim=1)
        y = self.linear(t_norm)
        return y.squeeze(1)

    def fit(self, X_train, y_train, X_val, y_val, epochs, lr):
        train_dataset = TensorDataset(X_train, y_train)
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        val_dataset = TensorDataset(X_val, y_val)
        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
        optimizer = Adam(self.parameters(), lr=lr)
        loss_function = EnhancedSignAgreementLoss(self.loss_penalty)

        best_val_loss = float('inf')
        best_model_state = None

        for epoch in range(epochs):
            self.train()
            total_train_loss = 0.0
            for x_batch, y_batch in train_loader:
                optimizer.zero_grad()
                outputs = self(x_batch)
                loss = loss_function(outputs, y_batch)
                loss.backward()
                optimizer.step()
                total_train_loss += loss.item() * x_batch.size(0)

            avg_train_loss = total_train_loss / len(train_loader.dataset)

            self.eval()
            total_val_loss = 0.0
            with torch.no_grad():
                for x_batch, y_batch in val_loader:
                    outputs = self(x_batch)
                    loss = loss_function(outputs, y_batch)
                    total_val_loss += loss.item() * x_batch.size(0)
            avg_val_loss = total_val_loss / len(val_loader.dataset)

            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                best_model_state = self.state_dict()

            if epoch % 10 == 0:
                print(f'Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')

        if best_model_state:
            self.load_state_dict(best_model_state)
            print(f'Best Validation Loss: {best_val_loss:.4f}')

    def predict(self, X):
        self.eval()
        with torch.no_grad():
            return self(X)

"""## GA ANFIS custom loss"""

class GAoptimizer_ANFIS_custom:
    def __init__(self, model_class, input_dim, train_dataset, val_dataset, epochs, population_size, max_generations, loss_penalty):
        self.model_class = model_class
        self.input_dim = input_dim
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset
        self.epochs = epochs
        self.population_size = population_size
        self.max_generations = max_generations
        self.loss_penalty = loss_penalty
        self.population = self.initialize_population()
        self.best_model_state = None
        self.best_validation_loss = float('inf')

    def initialize_population(self):
        return [{
            'functions': np.random.choice([3, 5, 7]),
            'lr': np.random.uniform(0.001, 0.01)
        } for _ in range(self.population_size)]

    def evaluate_individual(self, individual):
        model = self.model_class(functions=individual['functions'], input_dim=self.input_dim, loss_penalty=self.loss_penalty)
        validation_loss = model.fit(self.train_dataset, self.val_dataset, self.epochs, individual['lr'])

        if validation_loss < self.best_validation_loss:
            self.best_validation_loss = validation_loss
            self.best_model_state = (individual, model.state_dict())  # Save best hyperparameters and model state

        return validation_loss

    def evolve(self):
        for generation in range(self.max_generations):
            print(f'Generation {generation+1}')
            fitnesses = [self.evaluate_individual(ind) for ind in self.population]
            ranked_population = sorted(zip(self.population, fitnesses), key=lambda x: x[1])
            self.population = [ind for ind, fit in ranked_population[:int(len(ranked_population) / 2)]]

            while len(self.population) < self.population_size:
                parent1, parent2 = np.random.choice(self.population, 2, replace=False)
                child1, child2 = self.crossover(parent1, parent2)
                self.population.extend([self.mutate(child1), self.mutate(child2)])

    def crossover(self, parent1, parent2):
        child1, child2 = deepcopy(parent1), deepcopy(parent2)
        crossover_point = np.random.choice(list(child1.keys()))
        child1[crossover_point], child2[crossover_point] = child2[crossover_point], child1[crossover_point]
        return child1, child2

    def mutate(self, individual):
        mutation_property = np.random.choice(list(individual.keys()))
        if mutation_property == 'functions':
            individual[mutation_property] = np.random.choice([3, 5, 7])
        elif mutation_property == 'lr':
            individual[mutation_property] = np.random.uniform(0.001, 0.01)
        return individual

    def get_best_model(self):
        # Retrieve the best model using stored hyperparameters and state
        best_hyperparameters, best_state_dict = self.best_model_state
        model = self.model_class(functions=best_hyperparameters['functions'], input_dim=self.input_dim, loss_penalty=self.loss_penalty)
        model.load_state_dict(best_state_dict)
        return model